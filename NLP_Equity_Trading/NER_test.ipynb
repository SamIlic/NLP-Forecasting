{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import io\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import Embedding, LSTM, Dense, TimeDistributed, Dropout, Bidirectional, Input, concatenate, add, multiply\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Reshape, GlobalMaxPooling1D, Highway, Permute, Lambda\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "session = tf.Session(config = tf.ConfigProto(inter_op_parallelism_threads = 0,\n",
    "                                             intra_op_parallelism_threads = 0,\n",
    "                                             log_device_placement = True))\n",
    "\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading FASTTEXT english model bin\n",
    "word_embeddings_path = 'embeddings\\cc.en.300.bin'\n",
    "lang_model = FastText.load_fasttext_format(word_embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conll2003\\eng.train.txt 14987 sentences\n",
      "conll2003\\eng.testa.txt 3466 sentences\n",
      "conll2003\\eng.testb.txt 3684 sentences\n"
     ]
    }
   ],
   "source": [
    "MAX_COLUMNS = 2\n",
    "WORD_COL_NUM = 0\n",
    "LABEL_COL_NUM = 3\n",
    "\n",
    "Vocabulary = set()\n",
    "\n",
    "def read_file(file_path, vocab):\n",
    "       \n",
    "    corpus_sentences = []\n",
    "    input_sentence = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f_in:\n",
    "        for line in f_in:\n",
    "            line = line.strip()\n",
    "\n",
    "            if len(line) == 0 or line[0] == '#':\n",
    "                if len(input_sentence) > 0:\n",
    "                    corpus_sentences.append(input_sentence)\n",
    "                    input_sentence = []\n",
    "                continue\n",
    "            \n",
    "            line_splt = line.split(' ')\n",
    "            if len(line_splt) < MAX_COLUMNS:\n",
    "                continue\n",
    "            \n",
    "            vocab.add(line_splt[0])\n",
    "            input_sentence.append(line_splt)\n",
    "\n",
    "    if len(input_sentence) > 0:\n",
    "        corpus_sentences.append(input_sentence)\n",
    "\n",
    "    print(file_path, len(corpus_sentences), \"sentences\")\n",
    "    return corpus_sentences\n",
    "\n",
    "#Пропишите путь к частям корпуса CoNLL-2003\n",
    "train_path = 'conll2003\\\\eng.train.txt'\n",
    "train_sentences = read_file(train_path, Vocabulary)\n",
    "\n",
    "dev_path = 'conll2003\\\\eng.testa.txt'\n",
    "dev_sentences = read_file(dev_path, Vocabulary)\n",
    "\n",
    "test_path = 'conll2003\\\\eng.testb.txt'\n",
    "test_sentences = read_file(test_path, Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "word_embeddings = []\n",
    "embedding_size = len(lang_model['size'])\n",
    "\n",
    "word2idx[\"PADDING_TOKEN\"] = len(word2idx)\n",
    "word_embeddings.append(np.zeros(embedding_size))\n",
    "\n",
    "word2idx[\"UNKNOWN_TOKEN\"] = len(word2idx)\n",
    "word_embeddings.append(np.random.uniform(-0.25, 0.25, embedding_size))\n",
    "\n",
    "for token in Vocabulary:\n",
    "    if token not in word2idx:\n",
    "        try:\n",
    "            word_embeddings.append(lang_model[token])\n",
    "            word2idx[token] = len(word2idx)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "word_embeddings = np.array(word_embeddings, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {}\n",
    "char2idx[\"PADDING_TOKEN\"] = 0\n",
    "\n",
    "for token in Vocabulary:\n",
    "    for char in list(token):\n",
    "        if char not in char2idx:\n",
    "            char2idx[char] = len(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-MISC': 0, 'I-ORG': 1, 'I-PER': 2, 'B-ORG': 3, 'PADDING_LABEL': 4, 'O': 5, 'B-LOC': 6, 'I-LOC': 7, 'B-MISC': 8}\n"
     ]
    }
   ],
   "source": [
    "label_set = set()\n",
    "label_set.add('PADDING_LABEL')\n",
    "for dataset in [train_sentences, dev_sentences, test_sentences]:\n",
    "    for sentence in dataset:\n",
    "        for token in sentence:\n",
    "            label = token[LABEL_COL_NUM]\n",
    "            label_set.add(label)    \n",
    "\n",
    "label2idx = {}\n",
    "idx2label = {}\n",
    "for label in label_set:\n",
    "    label2idx[label] = len(label2idx)\n",
    "    \n",
    "print(label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204567 tokens, 605 unknown, 0.296%\n",
      "51578 tokens, 93 unknown, 0.18%\n",
      "46666 tokens, 155 unknown, 0.332%\n",
      "\n",
      "[array([    0, 20165,     0]), array([4, 5, 4])]\n",
      "\n",
      "[array([    0,  9038,  6764,  4327, 26582,  4854, 21114, 24727, 26393,\n",
      "       17773,     0]), array([4, 1, 5, 0, 5, 5, 5, 0, 5, 5, 4])]\n",
      "\n",
      "[array([   0, 7385, 5006,    0]), array([4, 2, 2, 4])]\n",
      "\n",
      "[array([    0, 18148, 20407,     0]), array([4, 7, 5, 4])]\n"
     ]
    }
   ],
   "source": [
    "cnn_len = 16\n",
    "\n",
    "def create_matrices(sentences, word2idx, label2idx):   \n",
    "    \n",
    "    unknown_idx = word2idx['UNKNOWN_TOKEN']\n",
    "    padding_idx = word2idx['PADDING_TOKEN'] \n",
    "    \n",
    "    padding_label = label2idx['PADDING_LABEL']  \n",
    "    \n",
    "    dataset = []\n",
    "    total_tokens = 0\n",
    "    unknown_tokens = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        \n",
    "        proper_sentence_start = 1\n",
    "\n",
    "        word_indices = np.array([padding_idx] * (len(sentence) + 2))\n",
    "        label_indices = np.array([padding_label] * (len(sentence) + 2))\n",
    "        \n",
    "        char_codes = [[0] * cnn_len]\n",
    "        \n",
    "        for pos_in_sentence, word in enumerate(sentence):\n",
    "\n",
    "            token_unknown, word_idx = get_token_indices(word, word2idx, unknown_idx)\n",
    "\n",
    "            pos_in_padded_sentence = pos_in_sentence + proper_sentence_start\n",
    "            \n",
    "            word_indices[pos_in_padded_sentence] = word_idx\n",
    "            label_indices[pos_in_padded_sentence] = label2idx[word[LABEL_COL_NUM]]\n",
    "\n",
    "            total_tokens += 1\n",
    "            if token_unknown:\n",
    "                unknown_tokens += 1\n",
    "            \n",
    "            st = word[WORD_COL_NUM][:cnn_len]\n",
    "            char_codes.append([char2idx[char] for char in list(st)] + [char2idx[\"PADDING_TOKEN\"]] * (cnn_len - len(st)))\n",
    "            \n",
    "        char_codes.append([0] * cnn_len)\n",
    "        char_records = np.array(char_codes)\n",
    "        \n",
    "        dataset.append([word_indices, label_indices, char_records])\n",
    "        \n",
    "    percent = 0.0\n",
    "    if total_tokens != 0:\n",
    "        percent = float(unknown_tokens) / total_tokens * 100\n",
    "    print(\"{} tokens, {} unknown, {:.3}%\".format(total_tokens, unknown_tokens, percent ))\n",
    "    return dataset\n",
    "\n",
    "def get_token_indices(token, word2idx, unknown_idx):\n",
    "\n",
    "    token_unknown = False\n",
    "    \n",
    "    word = token[WORD_COL_NUM]\n",
    "    \n",
    "    if word2idx.get(word) is not None:\n",
    "        word_idx = word2idx[word]\n",
    "    else:\n",
    "        word_idx = unknown_idx\n",
    "        token_unknown = True\n",
    "\n",
    "    return token_unknown, word_idx\n",
    "\n",
    "train_data = create_matrices(train_sentences, word2idx, label2idx)\n",
    "dev_data = create_matrices(dev_sentences, word2idx, label2idx)\n",
    "test_data = create_matrices(test_sentences, word2idx, label2idx)\n",
    "\n",
    "for sentence in train_data[:4]:\n",
    "    print()\n",
    "    print(sentence[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\layers.py:200: UserWarning: The `Highway` layer is deprecated and will be removed after 06/2017.\n",
      "  warnings.warn('The `Highway` layer is deprecated '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 16)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding_layer (Embedding (None, None, 16, 50) 4300        char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "char_cnn (TimeDistributed)      (None, None, 12, 200 50200       char_embedding_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "char_activation (TimeDistribute (None, None, 12, 200 2400        char_cnn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "char_pooling (TimeDistributed)  (None, None, 200)    0           char_activation[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "token_input (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_highway (TimeDistributed)  (None, None, 200)    80400       char_pooling[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "token_embeddings (Embedding)    (None, None, 300)    8973600     token_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "chars (TimeDistributed)         (None, None, 200)    0           char_highway[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "merged_embeddings (Concatenate) (None, None, 500)    0           token_embeddings[0][0]           \n",
      "                                                                 chars[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "blstm (Bidirectional)           [(None, None, 200),  480800      merged_embeddings[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "state_h_concat (Concatenate)    (None, 200)          0           blstm[0][1]                      \n",
      "                                                                 blstm[0][3]                      \n",
      "__________________________________________________________________________________________________\n",
      "state_h (Lambda)                (None, 1, 200)       0           state_h_concat[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_W1 (TimeDistributed)  (None, None, 100)    20100       blstm[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention_W2 (TimeDistributed)  (None, 1, 100)       20100       state_h[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_W (Add)               (None, None, 100)    0           attention_W1[0][0]               \n",
      "                                                                 attention_W2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_scores (Lambda)       (None, None, 100)    0           attention_W[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_V (TimeDistributed)   (None, None, 1)      101         attention_scores[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Lambda)      (None, None, 1)      0           attention_V[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "context_agg (Concatenate)       (None, None, 201)    0           blstm[0][0]                      \n",
      "                                                                 attention_weights[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "context (TimeDistributed)       (None, None, 201)    0           context_agg[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (TimeDistributed)         (None, None, 9)      1818        context[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (TimeDistributed)    (None, None, 9)      9           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "result (TimeDistributed)        (None, None, 9)      90          activation[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 9,633,918\n",
      "Trainable params: 660,318\n",
      "Non-trainable params: 8,973,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_LSTM_DIM = 100\n",
    "CNN_FILTERS = 200\n",
    "CNN_WIN = 5\n",
    "dim_char = 50\n",
    "\n",
    "n_out = len(label2idx)\n",
    "\n",
    "token_input = Input(dtype='int32', shape=(None,), name='token_input')\n",
    "token_embedding_layer = Embedding(input_dim=word_embeddings.shape[0], \n",
    "                                   output_dim=word_embeddings.shape[1],\n",
    "                                   weights=[word_embeddings], trainable=False, \n",
    "                                   name='token_embeddings')\n",
    "tokens = token_embedding_layer(token_input)\n",
    "\n",
    "char_input = Input(dtype='int32', shape=(None, cnn_len), name='char_input')\n",
    "char_embedding_layer = Embedding(input_dim=len(char2idx), output_dim=dim_char, name='char_embedding_layer')\n",
    "char_embeddings = char_embedding_layer(char_input)\n",
    "\n",
    "char_cnn = TimeDistributed(Conv1D(filters=CNN_FILTERS, kernel_size=CNN_WIN), name='char_cnn')(char_embeddings)\n",
    "char_activation = TimeDistributed(PReLU(), name='char_activation')(char_cnn)\n",
    "char_pooling = TimeDistributed(GlobalMaxPooling1D(), name='char_pooling')(char_activation)\n",
    "char_highway = TimeDistributed(Highway(), name='char_highway')(char_pooling)\n",
    "chars = TimeDistributed(Dropout(0.3), name = \"chars\")(char_highway)\n",
    "\n",
    "merged_embeddings = concatenate([tokens, chars], name='merged_embeddings')\n",
    "\n",
    "blstm, forward_h, forward_c, backward_h, backward_c = Bidirectional(\n",
    "    LSTM(SENTENCE_LSTM_DIM, return_sequences=True, return_state=True, implementation=2), name='blstm')(merged_embeddings)\n",
    "\n",
    "state_h_concat = concatenate([forward_h, backward_h], name = 'state_h_concat')\n",
    "state_h = Lambda(lambda x: tf.expand_dims(x, axis = 1), name = 'state_h')(state_h_concat)\n",
    "\n",
    "attention_W1 = TimeDistributed(Dense(SENTENCE_LSTM_DIM), name = 'attention_W1')(blstm)\n",
    "attention_W2 = TimeDistributed(Dense(SENTENCE_LSTM_DIM), name = 'attention_W2')(state_h)\n",
    "attention_W = add([attention_W1, attention_W2], name = 'attention_W')\n",
    "\n",
    "attention_scores = Lambda(lambda x: tf.nn.tanh(x), name = 'attention_scores')(attention_W)\n",
    "attention_V = TimeDistributed(Dense(1), name = 'attention_V')(attention_scores)\n",
    "attention_weights = Lambda(lambda x: tf.nn.softmax(x, axis = 1), name = 'attention_weights')(attention_V)\n",
    "\n",
    "context_agg = concatenate([blstm, attention_weights], name = \"context_agg\")\n",
    "context = TimeDistributed(Dropout(0.3), name = \"context\")(context_agg)\n",
    "\n",
    "dense = TimeDistributed(Dense(n_out), name='dense')(context)\n",
    "activation = TimeDistributed(PReLU(), name='activation')(dense)\n",
    "\n",
    "result = TimeDistributed(Dense(n_out, activation='softmax'), name='result')(activation)\n",
    "\n",
    "model = Model(inputs=[token_input, char_input], outputs=result)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 epochs\n",
      "\n",
      "14987 train sentences\n",
      "3466 dev sentences\n",
      "3684 test sentences\n",
      "\n",
      "--------- Epoch 0 -----------\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "216.50 sec for training\n",
      "\n",
      "================================== Train Data ==================================\n",
      "f1 =  0.9279234843773455\n",
      "================================== Dev Data: ==================================\n",
      "f1 =  0.9196760040499494\n",
      "================================== Test Data: ==================================\n",
      "f1 =  0.8931863365262968\n",
      "\n",
      "54.38 sec for evaluation\n",
      "\n",
      "--------- Epoch 1 -----------\n",
      "203.63 sec for training\n",
      "\n",
      "================================== Train Data ==================================\n",
      "f1 =  0.9577313842328633\n",
      "================================== Dev Data: ==================================\n",
      "f1 =  0.9395395901846699\n",
      "================================== Test Data: ==================================\n",
      "f1 =  0.9192220714608774\n",
      "\n",
      "49.44 sec for evaluation\n",
      "\n",
      "--------- Epoch 2 -----------\n",
      "185.05 sec for training\n",
      "\n",
      "================================== Train Data ==================================\n",
      "f1 =  0.9635309278350517\n",
      "================================== Dev Data: ==================================\n",
      "f1 =  0.945062409781778\n",
      "================================== Test Data: ==================================\n",
      "f1 =  0.9118634837069982\n",
      "\n",
      "49.89 sec for evaluation\n",
      "\n",
      "--------- Epoch 3 -----------\n",
      "189.35 sec for training\n",
      "\n",
      "================================== Train Data ==================================\n",
      "f1 =  0.9734842015371479\n",
      "================================== Dev Data: ==================================\n",
      "f1 =  0.9474482061647297\n",
      "================================== Test Data: ==================================\n",
      "f1 =  0.9154739400486093\n",
      "\n",
      "49.77 sec for evaluation\n",
      "\n",
      "--------- Epoch 4 -----------\n",
      "197.48 sec for training\n",
      "\n",
      "================================== Train Data ==================================\n",
      "f1 =  0.977962257691814\n",
      "================================== Dev Data: ==================================\n",
      "f1 =  0.9505318250886375\n",
      "================================== Test Data: ==================================\n",
      "f1 =  0.9161940768746062\n",
      "\n",
      "59.66 sec for evaluation\n",
      "\n",
      "--------- Epoch 5 -----------\n",
      "189.11 sec for training\n",
      "\n",
      "================================== Train Data ==================================\n",
      "f1 =  0.9789920447032227\n",
      "================================== Dev Data: ==================================\n",
      "f1 =  0.9499957837929\n",
      "================================== Test Data: ==================================\n",
      "f1 =  0.9170548190612683\n",
      "\n",
      "49.64 sec for evaluation\n",
      "\n",
      "--------- Epoch 6 -----------\n",
      "175.60 sec for training\n",
      "\n",
      "================================== Train Data ==================================\n",
      "f1 =  0.9839952326224833\n",
      "================================== Dev Data: ==================================\n",
      "f1 =  0.9542132235570865\n",
      "================================== Test Data: ==================================\n",
      "f1 =  0.9204616623423102\n",
      "\n",
      "47.62 sec for evaluation\n",
      "\n",
      "--------- Epoch 7 -----------\n",
      "175.89 sec for training\n",
      "\n",
      "================================== Train Data ==================================\n",
      "f1 =  0.9856640749813612\n",
      "================================== Dev Data: ==================================\n",
      "f1 =  0.9528135251072419\n",
      "================================== Test Data: ==================================\n",
      "f1 =  0.9189966735592917\n",
      "\n",
      "47.40 sec for evaluation\n",
      "\n",
      "--------- Epoch 8 -----------\n",
      "177.61 sec for training\n",
      "\n",
      "================================== Train Data ==================================\n",
      "f1 =  0.9901370564877674\n",
      "================================== Dev Data: ==================================\n",
      "f1 =  0.9518571791183688\n",
      "================================== Test Data: ==================================\n",
      "f1 =  0.9204658300984021\n",
      "\n",
      "46.59 sec for evaluation\n",
      "\n",
      "--------- Epoch 9 -----------\n",
      "179.64 sec for training\n",
      "\n",
      "================================== Train Data ==================================\n",
      "f1 =  0.9908425299378829\n",
      "================================== Dev Data: ==================================\n",
      "f1 =  0.9540152155536771\n",
      "================================== Test Data: ==================================\n",
      "f1 =  0.9213017218065447\n",
      "\n",
      "47.45 sec for evaluation\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 10\n",
    "print(\"%d epochs\" % number_of_epochs)\n",
    "print()\n",
    "\n",
    "def iterate_minibatches(dataset):   \n",
    "    for sentence in dataset:\n",
    "        tokens, labels, chars = sentence     \n",
    "        labels = np.expand_dims(labels, -1) \n",
    "        yield np.asarray([tokens]), np.asarray([labels]), np.asarray([chars])\n",
    "        \n",
    "def tag_dataset(dataset):\n",
    "    predicted_labels = []\n",
    "    correct_labels = []\n",
    "    for tokens, labels, chars in dataset:\n",
    "        pred = model.predict_on_batch([np.asarray([tokens]), np.asarray([chars])])[0]\n",
    "        pred_labels = [el.tolist().index(max(el)) for el in pred]\n",
    "        predicted_labels.append(pred_labels)\n",
    "        correct_labels.append(labels)\n",
    "    return predicted_labels, correct_labels\n",
    "   \n",
    "def compute_f1(predictions, correct, padding_label, no_entity_label):\n",
    "    total_tokens = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    for guessed_sentence, correct_sentence in zip(predictions, correct):\n",
    "        assert (len(guessed_sentence) == len(correct_sentence)), \"Guessed and correct sentences do not match\"\n",
    "        \n",
    "        local_cnt, local_tps = 0, 0\n",
    "        for j in range(len(guessed_sentence)):\n",
    "            if (correct_sentence[j] != padding_label):\n",
    "                total_tokens += 1\n",
    "                \n",
    "                if (guessed_sentence[j] == no_entity_label):\n",
    "                    if (guessed_sentence[j] == correct_sentence[j]):\n",
    "                        pass\n",
    "                    else:\n",
    "                        false_negatives += 1\n",
    "                        \n",
    "                else:\n",
    "                    local_cnt += 1\n",
    "                    if (guessed_sentence[j] == correct_sentence[j]):\n",
    "                        local_tps += 1\n",
    "                        \n",
    "                    if (guessed_sentence[j] != guessed_sentence[j+1]):\n",
    "                        if (local_cnt == local_tps):\n",
    "                            true_positives += 1\n",
    "                        else:\n",
    "                            false_positives += 1\n",
    "                                    \n",
    "                        local_cnt, local_tps = 0, 0\n",
    "\n",
    "    if total_tokens == 0:\n",
    "        return float(0)\n",
    "    else:        \n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        return f1\n",
    "\n",
    "print(\"%d train sentences\" % len(train_data))\n",
    "print(\"%d dev sentences\" % len(dev_data))\n",
    "print(\"%d test sentences\" % len(test_data))\n",
    "\n",
    "padding_label = label2idx['PADDING_LABEL']\n",
    "no_entity_label = label2idx['O']\n",
    "\n",
    "for epoch in range(number_of_epochs):    \n",
    "    print()\n",
    "    print(\"--------- Epoch %d -----------\" % epoch)\n",
    "    random.shuffle(train_data)\n",
    "    \n",
    "    start_time = time.time()    \n",
    "    for batch in iterate_minibatches(train_data):\n",
    "        tokens, labels, chars = batch       \n",
    "        model.train_on_batch([tokens, chars], labels)   \n",
    "    print(\"%.2f sec for training\" % (time.time() - start_time))\n",
    "    print()\n",
    "    \n",
    "    #Train Dataset       \n",
    "    start_time = time.time()  \n",
    "    print(\"================================== Train Data ==================================\")\n",
    "    predicted_labels, correct_labels = tag_dataset(train_data)        \n",
    "    accuracy = compute_f1(predicted_labels, correct_labels, padding_label, no_entity_label)\n",
    "    print(\"f1 = \", accuracy)\n",
    "\n",
    "    #Dev Dataset \n",
    "    print(\"================================== Dev Data: ==================================\")\n",
    "    predicted_labels, correct_labels = tag_dataset(dev_data)  \n",
    "    accuracy = compute_f1(predicted_labels, correct_labels, padding_label, no_entity_label)\n",
    "    print(\"f1 = \", accuracy)\n",
    "\n",
    "    #Test Dataset \n",
    "    print(\"================================== Test Data: ==================================\")\n",
    "    predicted_labels, correct_labels = tag_dataset(test_data)  \n",
    "    accuracy = compute_f1(predicted_labels, correct_labels, padding_label, no_entity_label)\n",
    "    print(\"f1 = \", accuracy)\n",
    "    print()\n",
    "    print(\"%.2f sec for evaluation\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
