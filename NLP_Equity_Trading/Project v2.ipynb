{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\parentpoller.py:116: UserWarning: Parent poll failed.  If the frontend dies,\n",
      "                the kernel may be left running.  Please let us know\n",
      "                about your system (bitness, Python, etc.) at\n",
      "                ipython-dev@scipy.org\n",
      "  ipython-dev@scipy.org\"\"\")\n",
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import io\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import Embedding, LSTM, Dense, TimeDistributed, Dropout, Bidirectional, Input, concatenate, add, multiply\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Reshape, GlobalMaxPooling1D, Highway, Permute, Lambda\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from gensim.models.wrappers import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "session = tf.Session(config = tf.ConfigProto(inter_op_parallelism_threads = 0,\n",
    "                                             intra_op_parallelism_threads = 0,\n",
    "                                             log_device_placement = True))\n",
    "\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "005930-SE.xls was processed. Non ascii lines removed: 5 Total lines left: 182\n",
      "7203-TO.xls was processed. Non ascii lines removed: 10 Total lines left: 304\n",
      "AAPL-US.xls was processed. Non ascii lines removed: 10 Total lines left: 249\n",
      "ABBV-US.xls was processed. Non ascii lines removed: 6 Total lines left: 161\n",
      "ABT-US.xls was processed. Non ascii lines removed: 4 Total lines left: 228\n",
      "ACN-US.xls was processed. Non ascii lines removed: 5 Total lines left: 165\n",
      "AMGN-US.xls was processed. Non ascii lines removed: 3 Total lines left: 243\n",
      "AMZN-US.xls was processed. Non ascii lines removed: 11 Total lines left: 252\n",
      "BA-US.xls was processed. Non ascii lines removed: 11 Total lines left: 374\n",
      "BABA-US.xls was processed. Non ascii lines removed: 6 Total lines left: 145\n",
      "BAYN-XE.xls was processed. Non ascii lines removed: 22 Total lines left: 260\n",
      "BBL-US.xls was processed. Non ascii lines removed: 4 Total lines left: 195\n",
      "BP.-LN.xls was processed. Non ascii lines removed: 17 Total lines left: 303\n",
      "BRK'A-US.xls was processed. Non ascii lines removed: 14 Total lines left: 153\n",
      "C-US.xls was processed. Non ascii lines removed: 16 Total lines left: 277\n",
      "CSCO-US.xls was processed. Non ascii lines removed: 6 Total lines left: 162\n",
      "CVX-US.xls was processed. Non ascii lines removed: 6 Total lines left: 146\n",
      "FB-US.xls was processed. Non ascii lines removed: 11 Total lines left: 176\n",
      "FP-FR.xls was processed. Non ascii lines removed: 13 Total lines left: 203\n",
      "GE-US.xls was processed. Non ascii lines removed: 23 Total lines left: 364\n",
      "GOOGL-US.xls was processed. Non ascii lines removed: 16 Total lines left: 164\n",
      "HON-US.xls was processed. Non ascii lines removed: 5 Total lines left: 165\n",
      "HSBA-LN.xls was processed. Non ascii lines removed: 4 Total lines left: 217\n",
      "IBM-US.xls was processed. Non ascii lines removed: 8 Total lines left: 272\n",
      "INTC-US.xls was processed. Non ascii lines removed: 6 Total lines left: 160\n",
      "JNJ-US.xls was processed. Non ascii lines removed: 10 Total lines left: 291\n",
      "JPM-US.xls was processed. Non ascii lines removed: 8 Total lines left: 265\n",
      "MMM-US.xls was processed. Non ascii lines removed: 0 Total lines left: 140\n",
      "MRK-US.xls was processed. Non ascii lines removed: 11 Total lines left: 272\n",
      "MSFT-US.xls was processed. Non ascii lines removed: 10 Total lines left: 184\n",
      "NESN-VX.xls was processed. Non ascii lines removed: 26 Total lines left: 111\n",
      "NOVN-VX.xls was processed. Non ascii lines removed: 14 Total lines left: 342\n",
      "NOVO'B-KO.xls was processed. Non ascii lines removed: 10 Total lines left: 225\n",
      "ORCL-US.xls was processed. Non ascii lines removed: 6 Total lines left: 143\n",
      "PFE-US.xls was processed. Non ascii lines removed: 14 Total lines left: 416\n",
      "RDSA-AE.xls was processed. Non ascii lines removed: 27 Total lines left: 287\n",
      "ROG-VX.xls was processed. Non ascii lines removed: 20 Total lines left: 313\n",
      "SAN-MC.xls was processed. Non ascii lines removed: 27 Total lines left: 208\n",
      "SIE-XE.xls was processed. Non ascii lines removed: 4 Total lines left: 253\n",
      "T-US.xls was processed. Non ascii lines removed: 9 Total lines left: 193\n",
      "VOW3-XE.xls was processed. Non ascii lines removed: 7 Total lines left: 353\n",
      "WFC-US.xls was processed. Non ascii lines removed: 9 Total lines left: 167\n",
      "WMT-US.xls was processed. Non ascii lines removed: 10 Total lines left: 199\n",
      "XOM-US.xls was processed. Non ascii lines removed: 10 Total lines left: 204\n"
     ]
    }
   ],
   "source": [
    "# Reading dataset from files\n",
    "\n",
    "def isNaN(num):\n",
    "    return num != num\n",
    "\n",
    "def is_ascii(s):\n",
    "    return (len(s) == len(s.encode()))\n",
    "\n",
    "def read_pr(file, newline_token):\n",
    "    xl = pd.ExcelFile(file)\n",
    "    df = xl.parse(\"SignificantDevelopment\")\n",
    "\n",
    "    df['Release Date'] = df['Release Date'].apply(lambda x: x.split(\" \")[0])\n",
    "    \n",
    "    non_ascii_count = 0\n",
    "\n",
    "    for i in df.index:\n",
    "        if isNaN(df.loc[i, 'Headline']):\n",
    "            df.at[i, :] = np.nan\n",
    "        elif (not is_ascii(df.loc[i, 'Headline'])):\n",
    "                df.at[i, :] = np.nan\n",
    "                non_ascii_count += 1\n",
    "\n",
    "    df.dropna(inplace = True)\n",
    "    \n",
    "    for d in set(df['Release Date']):\n",
    "        df_d = df[df['Release Date'] == d]\n",
    "\n",
    "        if (len(df_d.index) > 1):\n",
    "            df_row = {}\n",
    "            df_row['Topic'] = newline_token.join(set(df_d['Topic'].values))\n",
    "            df_row['Release Date'] = d\n",
    "            df_row['Company'] = df_d['Company'].values[0]\n",
    "            df_row['Headline'] = newline_token.join(df_d['Headline'].values)\n",
    "            \n",
    "            df = df.drop(df_d.index)\n",
    "            df = df.append(df_row, ignore_index=True)\n",
    "    \n",
    "    return df, non_ascii_count\n",
    "\n",
    "def read_sd(file1, file2):\n",
    "    df1 = pd.read_excel(file1, sheet_name='Sheet1', skiprows=[0])\n",
    "    df2 = pd.read_excel(file2, sheet_name='Sheet1', skiprows=[0])\n",
    "    df = pd.concat([df1, df2], ignore_index=True, sort=True).copy().sort_values(by='Date')\n",
    "\n",
    "    if (len(df.index) != len(df1.index) + len(df2.index)):\n",
    "        print (\"Some rows are missed!\") \n",
    "\n",
    "    df.fillna(method='ffill', inplace = True)\n",
    "    df.fillna(df.mean(), inplace = True)\n",
    "\n",
    "    ci_s = [ci for c in df.columns for ci in c.split() if ci != 'Close']\n",
    "    stock_name = max(set(ci_s), key = ci_s.count)\n",
    "    df.columns = [c.replace(stock_name, '').strip() for c in df.columns]\n",
    "    \n",
    "    df['r_stock'] = (df['Close'] / df['Close'].shift(1) - 1.0)\n",
    "    df['r_index'] = (df['.SPX-US Close'] / df['.SPX-US Close'].shift(1) - 1.0)\n",
    "    df['Release Date'] = df['Date'].apply(lambda x: \"/\".join([x.split(\"/\")[1], x.split(\"/\")[2], x.split(\"/\")[0][-2:]]))\n",
    "    \n",
    "    df.dropna(inplace = True)\n",
    "    \n",
    "    cols = ['Release Date', 'r_stock', 'r_index', 'MFI', 'ForPE', 'SIP']\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "    \n",
    "    df['MFI'] = df['MFI'].shift(1)\n",
    "    df['ForPE'] = df['ForPE'].shift(1)\n",
    "    df['SIP'] = df['SIP'].shift(1)\n",
    "    \n",
    "    return df[cols]\n",
    "\n",
    "newline_token = \" nnnewlineee \"\n",
    "\n",
    "dfs = []\n",
    "for f in os.listdir(\"press_releases\"):\n",
    "    if f.endswith(\".xls\"):\n",
    "        df, non_ascii_count = read_pr(\"press_releases\\\\\" + f, newline_token)\n",
    "        \n",
    "        if (f not in os.listdir('stock_data\\\\10-14\\\\')) or (f not in os.listdir('stock_data\\\\15-19\\\\')):\n",
    "            print (\"Stock\", f ,\"was not found!\")\n",
    "            pass\n",
    "        sd = read_sd('stock_data\\\\10-14\\\\' + f, 'stock_data\\\\15-19\\\\' + f)\n",
    "        \n",
    "        dfs.append(df.merge(sd, on = 'Release Date', how='left'))       \n",
    "        print (f, \"was processed. Non ascii lines removed:\", non_ascii_count, \"Total lines left:\", len(df.index))\n",
    "\n",
    "data = pd.concat(dfs, ignore_index = True)\n",
    "data = data[~data['r_stock'].isnull()]\n",
    "data.fillna(data.mean(), inplace = True)\n",
    "\n",
    "del df, dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of companies:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['BP plc (ADR)', 'Merck & Co., Inc.', 'Siemens AG (ADR)', 'Bayer AG (ADR)', 'Chevron Corporation', 'Toyota Motor Corp (ADR)', 'Nestle SA', 'Banco Santander SA (ADR)', 'Samsung Electronics Co Ltd', 'Royal Dutch Shell Plc', 'AbbVie Inc', 'The Chase Manhattan Corp', 'Oracle Corporation', 'Pfizer, Inc. (OLD)', 'Honeywell International Inc.', '3M Co', 'HSBC Holdings plc (ADR)', 'Boeing Co', 'Alphabet Inc', 'Abbott Laboratories', 'Accenture Plc', 'Roche Holding Ltd. (ADR)', 'Wells Fargo & Co', 'International Business Machines Corp.', 'Microsoft Corporation', 'Johnson & Johnson (OLD)', 'Amazon.com, Inc.', 'Berkshire Hathaway Inc.', 'Exxon Corporation', 'Novartis AG (ADR)', 'Walmart Inc', 'Travelers Group Inc', 'Novo Nordisk A/S (ADR)', 'BHP Group PLC', 'Volkswagen AG (ADR)', 'Apple Inc.', 'Total SA (ADR)', 'Cisco Systems, Inc.', 'General Electric Company', 'Facebook Inc', 'AT&T Inc.', 'Alibaba Group Holding Ltd', 'Amgen, Inc.', 'Intel Corporation'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Company to index\n",
    "\n",
    "Company_num = len(set(data['Company']))\n",
    "\n",
    "Company2idx = {}\n",
    "Company_embeddings = np.identity(Company_num, float)\n",
    "\n",
    "for company in set(data['Company']):\n",
    "    if company not in Company2idx:\n",
    "        Company2idx[company] = len(Company2idx)\n",
    "\n",
    "print('List of companies:')\n",
    "Company2idx.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char vocabulary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['PADDING_TOKEN', 'NEWLINE_TOKEN', 'S', 'a', 'm', 's', 'u', 'n', 'g', ' ', 'E', 'l', 'e', 'c', 'y', 'F', 'i', 't', 'I', 'v', 'M', 'o', 'r', 'C', 'h', 'p', 'W', 'D', 'T', 'Y', '-', 'Q', '4', 'O', 'P', 'f', 'U', '1', '3', '6', '2', '5', 'B', 'K', 'H', 'x', 'L', 'w', '0', '9', ',', 'd', 'R', '$', '.', '8', 'A', 'k', 'N', 'b', 'G', 'X', 'J', \"'\", 'V', '&', 'q', '7', '/', 'z', 'j', '<', '>', ':', ';', '+', '%', '\"', 'Z', '(', ')', '`', '!', '[', ']', '*', '#', '_'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Char to index\n",
    "\n",
    "char2idx = {}\n",
    "char2idx[\"PADDING_TOKEN\"] = 0\n",
    "char2idx[\"NEWLINE_TOKEN\"] = 1\n",
    "\n",
    "# Token length for char CNN implementation\n",
    "cnn_len = 16\n",
    "\n",
    "for sentence in data['Headline'].values:\n",
    "    for char in list(sentence):\n",
    "        if char not in char2idx:\n",
    "            char2idx[char] = len(char2idx)\n",
    "\n",
    "print('Char vocabulary:')\n",
    "char2idx.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n"
     ]
    }
   ],
   "source": [
    "# Loading FASTTEXT english model bin\n",
    "\n",
    "word_embeddings_path = 'embeddings\\cc.en.300.bin'\n",
    "lang_model = FastText.load_fasttext_format(word_embeddings_path)\n",
    "\n",
    "embedding_size = len(lang_model['size'])\n",
    "print ('Embedding size:', embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Release Date</th>\n",
       "      <th>Company</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Topic</th>\n",
       "      <th>r_stock</th>\n",
       "      <th>r_index</th>\n",
       "      <th>MFI</th>\n",
       "      <th>ForPE</th>\n",
       "      <th>SIP</th>\n",
       "      <th>Headline_proc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/30/19</td>\n",
       "      <td>Samsung Electronics Co Ltd</td>\n",
       "      <td>Samsung Elec Says Facility Investment In Memor...</td>\n",
       "      <td>Other Earnings Pre-Announcement</td>\n",
       "      <td>0.019780</td>\n",
       "      <td>0.015549</td>\n",
       "      <td>72.672</td>\n",
       "      <td>8.507</td>\n",
       "      <td>1.239389</td>\n",
       "      <td>[samsung, elec, say, facility, investment, in,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/28/19</td>\n",
       "      <td>Samsung Electronics Co Ltd</td>\n",
       "      <td>Samsung Electro-Mechanics Q4 Operating Profit ...</td>\n",
       "      <td>Earnings Announcements</td>\n",
       "      <td>0.006704</td>\n",
       "      <td>-0.007847</td>\n",
       "      <td>66.051</td>\n",
       "      <td>8.225</td>\n",
       "      <td>1.239389</td>\n",
       "      <td>[samsung, electro-mechanics, q4, operating, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/23/19</td>\n",
       "      <td>Samsung Electronics Co Ltd</td>\n",
       "      <td>SK Hynix Expects Lower 2019 Capex, Demand Reco...</td>\n",
       "      <td>Other Earnings Pre-Announcement</td>\n",
       "      <td>-0.003559</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>60.384</td>\n",
       "      <td>7.588</td>\n",
       "      <td>1.239389</td>\n",
       "      <td>[sk, hynix, expects, lower, 2019, capex, ,, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11/30/18</td>\n",
       "      <td>Samsung Electronics Co Ltd</td>\n",
       "      <td>Samsung Faces $5.8 Bln Loss In Sales After Tec...</td>\n",
       "      <td>Regulatory / Company Investigation, Other Earn...</td>\n",
       "      <td>-0.030127</td>\n",
       "      <td>0.008185</td>\n",
       "      <td>59.982</td>\n",
       "      <td>6.330</td>\n",
       "      <td>1.239389</td>\n",
       "      <td>[samsung, face, $, 5.8, bln, loss, in, sale, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11/15/18</td>\n",
       "      <td>Samsung Electronics Co Ltd</td>\n",
       "      <td>Credit Suisse Says Surprised About Mobile Paym...</td>\n",
       "      <td>Regulatory / Company Investigation</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.010594</td>\n",
       "      <td>53.966</td>\n",
       "      <td>6.395</td>\n",
       "      <td>1.239389</td>\n",
       "      <td>[credit, suisse, say, surprise, about, mobile,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Release Date                     Company  \\\n",
       "0     01/30/19  Samsung Electronics Co Ltd   \n",
       "1     01/28/19  Samsung Electronics Co Ltd   \n",
       "2     01/23/19  Samsung Electronics Co Ltd   \n",
       "3     11/30/18  Samsung Electronics Co Ltd   \n",
       "4     11/15/18  Samsung Electronics Co Ltd   \n",
       "\n",
       "                                            Headline  \\\n",
       "0  Samsung Elec Says Facility Investment In Memor...   \n",
       "1  Samsung Electro-Mechanics Q4 Operating Profit ...   \n",
       "2  SK Hynix Expects Lower 2019 Capex, Demand Reco...   \n",
       "3  Samsung Faces $5.8 Bln Loss In Sales After Tec...   \n",
       "4  Credit Suisse Says Surprised About Mobile Paym...   \n",
       "\n",
       "                                               Topic   r_stock   r_index  \\\n",
       "0                    Other Earnings Pre-Announcement  0.019780  0.015549   \n",
       "1                             Earnings Announcements  0.006704 -0.007847   \n",
       "2                    Other Earnings Pre-Announcement -0.003559  0.002203   \n",
       "3  Regulatory / Company Investigation, Other Earn... -0.030127  0.008185   \n",
       "4                 Regulatory / Company Investigation  0.003401  0.010594   \n",
       "\n",
       "      MFI  ForPE       SIP                                      Headline_proc  \n",
       "0  72.672  8.507  1.239389  [samsung, elec, say, facility, investment, in,...  \n",
       "1  66.051  8.225  1.239389  [samsung, electro-mechanics, q4, operating, pr...  \n",
       "2  60.384  7.588  1.239389  [sk, hynix, expects, lower, 2019, capex, ,, de...  \n",
       "3  59.982  6.330  1.239389  [samsung, face, $, 5.8, bln, loss, in, sale, a...  \n",
       "4  53.966  6.395  1.239389  [credit, suisse, say, surprise, about, mobile,...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization of text tokens\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return {'pos': wordnet.ADJ}\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return {'pos': wordnet.VERB}\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return {'pos': wordnet.NOUN}\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return {'pos': wordnet.ADV}\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "headlines_processed = []\n",
    "\n",
    "for ind, row in data.iterrows():\n",
    "    sentence_clean = row['Headline'].replace('\"', '').replace(\"'\", '')\n",
    "    sentence_list = pos_tag(word_tokenize(sentence_clean))\n",
    "    sentence_parsed = [wordnet_lemmatizer.lemmatize(token.lower(), **get_wordnet_pos(pos)) for token, pos in sentence_list]\n",
    "    headlines_processed.append(sentence_parsed)\n",
    "\n",
    "data['Headline_proc'] = headlines_processed\n",
    "data.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.2075986 ,  0.06262114,  0.05612901, ..., -0.22106387,\n",
       "        -0.21707186,  0.11902369],\n",
       "       [-0.02436286, -0.06004606,  0.00459439, ...,  0.08166368,\n",
       "        -0.00982456, -0.04823476],\n",
       "       [-0.1724217 ,  0.04652646,  0.02433003, ...,  0.20093898,\n",
       "         0.01823837, -0.18172157]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word to embedding\n",
    "\n",
    "word2idx = {}\n",
    "word_embeddings = []\n",
    "\n",
    "# Initialization of embeddings for pads and OOV\n",
    "word2idx[\"PADDING_TOKEN\"] = len(word2idx)\n",
    "word_embeddings.append(np.zeros(embedding_size))\n",
    "\n",
    "word2idx[\"UNKNOWN_TOKEN\"] = len(word2idx)\n",
    "word_embeddings.append(np.random.uniform(-0.25, 0.25, embedding_size))\n",
    "\n",
    "# Получаем вектора токенов из словаря\n",
    "for sentence_list in data['Headline_proc'].values:\n",
    "    for token in sentence_list:\n",
    "        if token not in word2idx:\n",
    "            try:\n",
    "                word_embeddings.append(lang_model[token])\n",
    "                word2idx[token] = len(word2idx)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "word_embeddings = np.array(word_embeddings, dtype='float32')\n",
    "\n",
    "print ('Word embeddings:')\n",
    "word_embeddings[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 9235\n",
      "Test size: 486\n"
     ]
    }
   ],
   "source": [
    "# Train-dev-test split\n",
    "\n",
    "train_dataset = data.sample(frac=0.95, random_state=1234)\n",
    "test_dataset = data.drop(train_dataset.index)\n",
    "\n",
    "print ('Train size:', len(train_dataset.index))\n",
    "print ('Test size:', len(test_dataset.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 133517 tokens, 868 unknown, 0.65%\n",
      "Test data: 7040 tokens, 40 unknown, 0.568%\n"
     ]
    }
   ],
   "source": [
    "# Converting unstructured data to matrices\n",
    "\n",
    "def create_matrices(df, df_type):   \n",
    "    total_tokens = 0\n",
    "    unknown_tokens = 0\n",
    "    dataset = []\n",
    "    \n",
    "    for ind, row in df.iterrows():\n",
    "        \n",
    "        # Get company code\n",
    "        company_index = Company2idx[row['Company']]\n",
    "        \n",
    "        # Get word embedding indices\n",
    "        word_indices = [word2idx['PADDING_TOKEN']]\n",
    "        \n",
    "        for token in row['Headline_proc']:\n",
    "            if (token != newline_token):\n",
    "                total_tokens += 1\n",
    "                \n",
    "                if token in word2idx:\n",
    "                    word_idx = word2idx[token]\n",
    "                else:\n",
    "                    word_idx = word2idx[\"UNKNOWN_TOKEN\"]\n",
    "                    unknown_tokens += 1\n",
    "            else:\n",
    "                word_idx = word2idx['PADDING_TOKEN']\n",
    "            \n",
    "            word_indices.append(word_idx)\n",
    "        \n",
    "        word_indices.append(word2idx['PADDING_TOKEN'])\n",
    "        \n",
    "        # Get char indices\n",
    "        char_codes = [[char2idx[\"PADDING_TOKEN\"]] * cnn_len]\n",
    "        \n",
    "        for token in word_tokenize(row['Headline'].replace('\"', '').replace(\"'\", '')):\n",
    "            if (token != newline_token):\n",
    "                token_trunc = token[:cnn_len]\n",
    "                token_chars = [char2idx[char] for char in list(token_trunc)]\n",
    "                token_chars = token_chars + [char2idx[\"PADDING_TOKEN\"]] * (cnn_len - len(token_trunc))\n",
    "            else:\n",
    "                token_chars = [char2idx[\"PADDING_TOKEN\"]] * cnn_len\n",
    "                \n",
    "            char_codes.append(token_chars)\n",
    "            \n",
    "        char_codes.append([char2idx[\"PADDING_TOKEN\"]] * cnn_len)        \n",
    "        \n",
    "        # Get true label\n",
    "        label = row['r_stock']\n",
    "        \n",
    "        # Get numerical features\n",
    "        r_index = row['r_index']\n",
    "        MFI = row['MFI']\n",
    "        ForPE = row['ForPE']\n",
    "        SIP = row['SIP']        \n",
    "\n",
    "        # Save sample\n",
    "        dataset.append([company_index, np.array(word_indices), np.array(char_codes), r_index, MFI, ForPE, SIP, label])\n",
    "        \n",
    "    unknown_percent = 0.0\n",
    "    if total_tokens != 0:\n",
    "        unknown_percent = 100 * float(unknown_tokens) / total_tokens\n",
    "    print(df_type + \" data: {} tokens, {} unknown, {:.3}%\".format(total_tokens, unknown_tokens, unknown_percent))\n",
    "    \n",
    "    return np.array(dataset)\n",
    "\n",
    "train_data = create_matrices(train_dataset, 'Train')\n",
    "test_data = create_matrices(test_dataset, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\layers.py:200: UserWarning: The `Highway` layer is deprecated and will be removed after 06/2017.\n",
      "  warnings.warn('The `Highway` layer is deprecated '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 16)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding_layer (Embedding (None, None, 16, 32) 2816        char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "char_cnn (TimeDistributed)      (None, None, 12, 64) 10304       char_embedding_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "char_activation (TimeDistribute (None, None, 12, 64) 768         char_cnn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "char_pooling (TimeDistributed)  (None, None, 64)     0           char_activation[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "token_input (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_highway (TimeDistributed)  (None, None, 64)     8320        char_pooling[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "token_embeddings (Embedding)    (None, None, 300)    3920400     token_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "chars (TimeDistributed)         (None, None, 64)     0           char_highway[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "merged_embeddings (Concatenate) (None, None, 364)    0           token_embeddings[0][0]           \n",
      "                                                                 chars[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "blstm (Bidirectional)           [(None, None, 32), ( 48768       merged_embeddings[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "state_h_concat (Concatenate)    (None, 32)           0           blstm[0][1]                      \n",
      "                                                                 blstm[0][3]                      \n",
      "__________________________________________________________________________________________________\n",
      "state_h (Lambda)                (None, 1, 32)        0           state_h_concat[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_W1 (TimeDistributed)  (None, None, 16)     528         blstm[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention_W2 (TimeDistributed)  (None, 1, 16)        528         state_h[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_W (Add)               (None, None, 16)     0           attention_W1[0][0]               \n",
      "                                                                 attention_W2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_scores (Lambda)       (None, None, 16)     0           attention_W[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_V (TimeDistributed)   (None, None, 1)      17          attention_scores[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Lambda)      (None, None, 1)      0           attention_V[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "company_input (InputLayer)      (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "context_vector (Multiply)       (None, None, 32)     0           attention_weights[0][0]          \n",
      "                                                                 blstm[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "company_embeddings (Embedding)  (None, 1, 4)         176         company_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "context_agg (Lambda)            (None, 32)           0           context_vector[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "company (Lambda)                (None, 4)            0           company_embeddings[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "context (Dense)                 (None, 16)           528         context_agg[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "r_index (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "MFI (InputLayer)                (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ForPE (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "SIP (InputLayer)                (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "features (Concatenate)          (None, 24)           0           company[0][0]                    \n",
      "                                                                 context[0][0]                    \n",
      "                                                                 r_index[0][0]                    \n",
      "                                                                 MFI[0][0]                        \n",
      "                                                                 ForPE[0][0]                      \n",
      "                                                                 SIP[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "features_dropout (Dropout)      (None, 24)           0           features[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           400         features_dropout[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation (PReLU)              (None, 16)           16          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "result (Dense)                  (None, 1)            17          activation[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,993,586\n",
      "Trainable params: 73,186\n",
      "Non-trainable params: 3,920,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dim_HIDDEN = 16\n",
    "CNN_FILTERS = 64\n",
    "dim_company = 4\n",
    "dim_CHAR = 32\n",
    "CNN_WIN = 5\n",
    "\n",
    "# Input layers and embeddings\n",
    "company_input = Input(dtype='int32', shape=(1,), name='company_input')\n",
    "company_embedding_layer = Embedding(input_dim=len(Company2idx), output_dim=dim_company,\n",
    "                                    trainable=True, name='company_embeddings')\n",
    "company = company_embedding_layer(company_input)\n",
    "company = Lambda(lambda x: K.squeeze(x, 1), name='company')(company)\n",
    "\n",
    "r_index = Input(dtype='float32', shape=(1,), name='r_index')\n",
    "MFI = Input(dtype='float32', shape=(1,), name='MFI')\n",
    "ForPE = Input(dtype='float32', shape=(1,), name='ForPE')\n",
    "SIP = Input(dtype='float32', shape=(1,), name='SIP')\n",
    "\n",
    "token_input = Input(dtype='int32', shape=(None,), name='token_input')\n",
    "token_embedding_layer = Embedding(input_dim=word_embeddings.shape[0], \n",
    "                                   output_dim=word_embeddings.shape[1],\n",
    "                                   weights=[word_embeddings], trainable=False, \n",
    "                                   name='token_embeddings')\n",
    "tokens = token_embedding_layer(token_input)\n",
    "\n",
    "char_input = Input(dtype='int32', shape=(None, cnn_len), name='char_input')\n",
    "char_embedding_layer = Embedding(input_dim=len(char2idx), output_dim=dim_CHAR, name='char_embedding_layer')\n",
    "char_embeddings = char_embedding_layer(char_input)\n",
    "\n",
    "# Implementation of char CNN\n",
    "char_cnn = TimeDistributed(Conv1D(filters=CNN_FILTERS, kernel_size=CNN_WIN), name='char_cnn')(char_embeddings)\n",
    "char_activation = TimeDistributed(PReLU(), name='char_activation')(char_cnn)\n",
    "char_pooling = TimeDistributed(GlobalMaxPooling1D(), name='char_pooling')(char_activation)\n",
    "char_highway = TimeDistributed(Highway(), name='char_highway')(char_pooling)\n",
    "chars = TimeDistributed(Dropout(0.30), name = \"chars\")(char_highway)\n",
    "\n",
    "merged_embeddings = concatenate([tokens, chars], name='merged_embeddings')\n",
    "\n",
    "# Implementation of BLSTM\n",
    "blstm, forward_h, forward_c, backward_h, backward_c = Bidirectional(\n",
    "    LSTM(dim_HIDDEN, return_sequences=True, return_state=True, implementation=2), name='blstm')(merged_embeddings)\n",
    "\n",
    "# Implementation of attention\n",
    "state_h_concat = concatenate([forward_h, backward_h], name = 'state_h_concat')\n",
    "state_h = Lambda(lambda x: tf.expand_dims(x, axis = 1), name = 'state_h')(state_h_concat)\n",
    "\n",
    "attention_W1 = TimeDistributed(Dense(dim_HIDDEN), name = 'attention_W1')(blstm)\n",
    "attention_W2 = TimeDistributed(Dense(dim_HIDDEN), name = 'attention_W2')(state_h)\n",
    "attention_W = add([attention_W1, attention_W2], name = 'attention_W')\n",
    "\n",
    "attention_scores = Lambda(lambda x: tf.nn.tanh(x), name = 'attention_scores')(attention_W)\n",
    "attention_V = TimeDistributed(Dense(1), name = 'attention_V')(attention_scores)\n",
    "attention_weights = Lambda(lambda x: tf.nn.softmax(x, axis = 1), name = 'attention_weights')(attention_V)\n",
    "\n",
    "# Weighting context embeddings by attention\n",
    "context_vector = multiply([attention_weights, blstm], name = \"context_vector\")\n",
    "context_agg = Lambda(lambda x: tf.reduce_sum(x, axis=1), name = \"context_agg\")(context_vector)\n",
    "context = Dense(dim_HIDDEN, name='context')(context_agg)\n",
    "\n",
    "# Combining vector of features\n",
    "features = concatenate([company, context, r_index, MFI, ForPE, SIP], name = 'features')\n",
    "features_dropout = Dropout(0.30, name = \"features_dropout\")(features)\n",
    "\n",
    "# Output regression\n",
    "dense = Dense(dim_HIDDEN, name='dense')(features_dropout)\n",
    "activation = PReLU(name='activation')(dense)\n",
    "result = Dense(1, name='result')(activation)\n",
    "\n",
    "# Compiling model\n",
    "model = Model(inputs=[company_input, token_input, char_input, r_index, MFI, ForPE, SIP], outputs=result)\n",
    "model.compile(loss='mse', optimizer=Adam())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 epochs\n",
      "\n",
      "9235 train sentences\n",
      "486 test sentences\n",
      "\n",
      "--------- Epoch 0 -----------\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "72.68 sec for training\n",
      "\n",
      "================================== Train Data ==================================\n",
      "RMSE =  0.27435272143270856\n",
      "================================== Test Data: ==================================\n",
      "RMSE =  0.2669889999616818\n",
      "\n",
      "15.13 sec for evaluation\n",
      "\n",
      "--------- Epoch 1 -----------\n",
      "65.35 sec for training\n",
      "\n",
      "================================== Train Data ==================================\n",
      "RMSE =  0.09505458342477073\n",
      "================================== Test Data: ==================================\n",
      "RMSE =  0.11046102768238672\n",
      "\n",
      "18.66 sec for evaluation\n",
      "\n",
      "--------- Epoch 2 -----------\n",
      "67.69 sec for training\n",
      "\n",
      "================================== Train Data ==================================\n",
      "RMSE =  0.019311870189981508\n",
      "================================== Test Data: ==================================\n",
      "RMSE =  0.02444978597783986\n",
      "\n",
      "17.43 sec for evaluation\n",
      "\n",
      "--------- Epoch 3 -----------\n",
      "67.98 sec for training\n",
      "\n",
      "================================== Train Data ==================================\n",
      "RMSE =  0.014523009180562358\n",
      "================================== Test Data: ==================================\n",
      "RMSE =  0.020994951059156303\n",
      "\n",
      "17.01 sec for evaluation\n",
      "\n",
      "--------- Epoch 4 -----------\n",
      "66.66 sec for training\n",
      "\n",
      "================================== Train Data ==================================\n",
      "RMSE =  0.013693130099892223\n",
      "================================== Test Data: ==================================\n",
      "RMSE =  0.021128044641349507\n",
      "\n",
      "15.70 sec for evaluation\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 5\n",
    "lr_decay = 0.5\n",
    "K.set_value(model.optimizer.lr, 0.01)\n",
    "random.seed(1234)\n",
    "print(\"%d epochs\" % number_of_epochs)\n",
    "print()\n",
    "\n",
    "def iterate_minibatches(dataset):   \n",
    "    for sentence in dataset:\n",
    "        companies, tokens, chars, r_index, MFI, ForPE, SIP, label = sentence     \n",
    "        yield (np.asarray([companies]), np.asarray([tokens]), np.asarray([chars]), np.asarray([r_index]), \n",
    "               np.asarray([MFI]), np.asarray([ForPE]), np.asarray([SIP]), np.asarray([label]))\n",
    "\n",
    "def tag_dataset(dataset):\n",
    "    predicted_returns = []\n",
    "    true_returns = []\n",
    "    for company, tokens, chars, r_index, MFI, ForPE, SIP, label in dataset:\n",
    "        pred = model.predict_on_batch([np.asarray([company]), np.asarray([tokens]), np.asarray([chars]), np.asarray([r_index]),\n",
    "                                       np.asarray([MFI]), np.asarray([ForPE]), np.asarray([SIP])])[0]\n",
    "        predicted_returns.append(pred)\n",
    "        true_returns.append(label)\n",
    "    return predicted_returns, true_returns\n",
    "\n",
    "def compute_rmse(y_pred, y_true):\n",
    "    return np.sqrt(mean_squared_error(y_true = y_true, y_pred = y_pred))\n",
    "\n",
    "print(\"%d train sentences\" % len(train_data))\n",
    "print(\"%d test sentences\" % len(test_data))\n",
    "\n",
    "for epoch in range(number_of_epochs):    \n",
    "    print()\n",
    "    print(\"--------- Epoch %d -----------\" % epoch)\n",
    "    random.shuffle(train_data)\n",
    "    \n",
    "    start_time = time.time()    \n",
    "    for batch in iterate_minibatches(train_data):\n",
    "        companies, tokens, chars, r_index, MFI, ForPE, SIP, label = batch       \n",
    "        model.train_on_batch([companies, tokens, chars, r_index, MFI, ForPE, SIP], label)   \n",
    "    print(\"%.2f sec for training\" % (time.time() - start_time))\n",
    "    print()\n",
    "    \n",
    "    #Train Dataset       \n",
    "    start_time = time.time()  \n",
    "    print(\"================================== Train Data ==================================\")    \n",
    "    predicted, correct = tag_dataset(train_data)  \n",
    "    RMSE = compute_rmse(predicted, correct)\n",
    "    print(\"RMSE = \", RMSE)\n",
    "\n",
    "    #Test Dataset \n",
    "    print(\"================================== Test Data: ==================================\")\n",
    "    predicted, correct = tag_dataset(test_data)  \n",
    "    RMSE = compute_rmse(predicted, correct)\n",
    "    print(\"RMSE = \", RMSE)\n",
    "    print()\n",
    "    print(\"%.2f sec for evaluation\" % (time.time() - start_time))\n",
    "    \n",
    "    current_lr = K.get_value(model.optimizer.lr)\n",
    "    K.set_value(model.optimizer.lr, current_lr * (1.0 - lr_decay))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "company_input (InputLayer)      (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "company_embeddings (Embedding)  (None, 1, 4)         176         company_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "company (Lambda)                (None, 4)            0           company_embeddings[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "noise_vector (InputLayer)       (None, 16)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "r_index (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "MFI (InputLayer)                (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ForPE (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "SIP (InputLayer)                (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "features (Concatenate)          (None, 24)           0           company[0][0]                    \n",
      "                                                                 noise_vector[0][0]               \n",
      "                                                                 r_index[0][0]                    \n",
      "                                                                 MFI[0][0]                        \n",
      "                                                                 ForPE[0][0]                      \n",
      "                                                                 SIP[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "features_dropout (Dropout)      (None, 24)           0           features[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           400         features_dropout[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation (PReLU)              (None, 16)           16          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "result (Dense)                  (None, 1)            17          activation[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 609\n",
      "Trainable params: 609\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dim_HIDDEN = 16\n",
    "dim_company = 4\n",
    "\n",
    "# Input layers and embeddings\n",
    "company_input = Input(dtype='int32', shape=(1,), name='company_input')\n",
    "company_embedding_layer = Embedding(input_dim=len(Company2idx), output_dim=dim_company,\n",
    "                                    trainable=True, name='company_embeddings')\n",
    "company = company_embedding_layer(company_input)\n",
    "company = Lambda(lambda x: K.squeeze(x, 1), name='company')(company)\n",
    "\n",
    "noise_vector = Input(dtype='float32', shape=(dim_HIDDEN,), name='noise_vector')\n",
    "\n",
    "r_index = Input(dtype='float32', shape=(1,), name='r_index')\n",
    "MFI = Input(dtype='float32', shape=(1,), name='MFI')\n",
    "ForPE = Input(dtype='float32', shape=(1,), name='ForPE')\n",
    "SIP = Input(dtype='float32', shape=(1,), name='SIP')\n",
    "\n",
    "# Combining vector of features\n",
    "features = concatenate([company, noise_vector, r_index, MFI, ForPE, SIP], name = 'features')\n",
    "features_dropout = Dropout(0.30, name = \"features_dropout\")(features)\n",
    "\n",
    "# Output regression\n",
    "dense = Dense(dim_HIDDEN, name='dense')(features_dropout)\n",
    "activation = PReLU(name='activation')(dense)\n",
    "result = Dense(1, name='result')(activation)\n",
    "\n",
    "# Compiling model\n",
    "model = Model(inputs=[company_input, noise_vector, r_index, MFI, ForPE, SIP], outputs=result)\n",
    "model.compile(loss='mse', optimizer=Adam())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 epochs\n",
      "\n",
      "9235 train observations\n",
      "486 test observations\n",
      "\n",
      "--------- Epoch 0 -----------\n",
      "12.09 sec for training\n",
      "\n",
      "================================== Train Data ==================================\n",
      "RMSE =  0.0020940749952854418\n",
      "================================== Test Data: ==================================\n",
      "RMSE =  0.024600661473915614\n",
      "\n",
      "7.61 sec for evaluation\n",
      "\n",
      "--------- Epoch 1 -----------\n",
      "8.58 sec for training\n",
      "\n",
      "================================== Train Data ==================================\n",
      "RMSE =  0.00047133445018445136\n",
      "================================== Test Data: ==================================\n",
      "RMSE =  0.025216087776783278\n",
      "\n",
      "6.58 sec for evaluation\n",
      "\n",
      "--------- Epoch 2 -----------\n",
      "8.71 sec for training\n",
      "\n",
      "================================== Train Data ==================================\n",
      "RMSE =  0.0002665597918634258\n",
      "================================== Test Data: ==================================\n",
      "RMSE =  0.024591328508430592\n",
      "\n",
      "6.56 sec for evaluation\n",
      "\n",
      "--------- Epoch 3 -----------\n",
      "8.83 sec for training\n",
      "\n",
      "================================== Train Data ==================================\n",
      "RMSE =  6.831015763747323e-05\n",
      "================================== Test Data: ==================================\n",
      "RMSE =  0.024664564952236127\n",
      "\n",
      "6.64 sec for evaluation\n",
      "\n",
      "--------- Epoch 4 -----------\n",
      "8.67 sec for training\n",
      "\n",
      "================================== Train Data ==================================\n",
      "RMSE =  0.002510423378195602\n",
      "================================== Test Data: ==================================\n",
      "RMSE =  0.02362537984107042\n",
      "\n",
      "6.40 sec for evaluation\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 5\n",
    "lr_decay = 0.5\n",
    "K.set_value(model.optimizer.lr, 0.01)\n",
    "random.seed(1234)\n",
    "print(\"%d epochs\" % number_of_epochs)\n",
    "print()\n",
    "\n",
    "random_train = np.random.normal(size=(len(train_dataset.index), dim_HIDDEN))\n",
    "random_test = np.random.normal(size=(len(test_dataset.index), dim_HIDDEN))\n",
    "\n",
    "def iterate_minibatches(dataset, random_vecs):\n",
    "    i = -1\n",
    "    for sentence in dataset:\n",
    "        i += 1\n",
    "        companies, _, _, r_index, MFI, ForPE, SIP, label = sentence     \n",
    "        yield (np.asarray([companies]), np.asarray([r_index]), \n",
    "               np.asarray([MFI]), np.asarray([ForPE]), np.asarray([SIP]), np.asarray([label]), np.asarray([random_vecs[i]]))\n",
    "\n",
    "def tag_dataset(dataset, random_vecs):\n",
    "    predicted_returns = []\n",
    "    true_returns = []\n",
    "    i = -1\n",
    "    for company, _, _, r_index, MFI, ForPE, SIP, label in dataset:\n",
    "        i += 1\n",
    "        pred = model.predict_on_batch([np.asarray([company]), np.asarray([random_vecs[i]]), np.asarray([r_index]),\n",
    "                                       np.asarray([MFI]), np.asarray([ForPE]), np.asarray([SIP])])[0]\n",
    "        predicted_returns.append(pred)\n",
    "        true_returns.append(label)\n",
    "    return predicted_returns, true_returns\n",
    "\n",
    "def compute_rmse(y_pred, y_true):\n",
    "    return np.sqrt(mean_squared_error(y_true = y_true, y_pred = y_pred))\n",
    "\n",
    "print(\"%d train observations\" % len(train_data))\n",
    "print(\"%d test observations\" % len(test_data))\n",
    "\n",
    "for epoch in range(number_of_epochs):    \n",
    "    print()\n",
    "    print(\"--------- Epoch %d -----------\" % epoch)\n",
    "    random.shuffle(train_data)\n",
    "    \n",
    "    start_time = time.time()    \n",
    "    for batch in iterate_minibatches(train_data, random_train):\n",
    "        companies, r_index, MFI, ForPE, SIP, label, random_vec = batch       \n",
    "        model.train_on_batch([companies, random_vec, r_index, MFI, ForPE, SIP], label)   \n",
    "    print(\"%.2f sec for training\" % (time.time() - start_time))\n",
    "    print()\n",
    "    \n",
    "    #Train Dataset       \n",
    "    start_time = time.time()  \n",
    "    print(\"================================== Train Data ==================================\")    \n",
    "    predicted, correct = tag_dataset(train_data, random_train)  \n",
    "    RMSE = compute_rmse(predicted, correct)\n",
    "    print(\"RMSE = \", RMSE)\n",
    "\n",
    "    #Test Dataset \n",
    "    print(\"================================== Test Data: ==================================\")\n",
    "    predicted, correct = tag_dataset(test_data, random_test)  \n",
    "    RMSE = compute_rmse(predicted, correct)\n",
    "    print(\"RMSE = \", RMSE)\n",
    "    print()\n",
    "    \n",
    "    print(\"%.2f sec for evaluation\" % (time.time() - start_time))\n",
    "    \n",
    "    current_lr = K.get_value(model.optimizer.lr)\n",
    "    K.set_value(model.optimizer.lr, current_lr * (1.0 - lr_decay))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=False, input='content', stop_words='english',\n",
    "                             ngram_range=(1,1), tokenizer=identity_tokenizer)\n",
    "trSVD = TruncatedSVD(n_components=500, n_iter=100)\n",
    "\n",
    "tv = vectorizer.fit_transform(list(data['Headline_proc'].values))\n",
    "tr_tv = trSVD.fit_transform(tv)\n",
    "\n",
    "data['co_dummy'] = pd.get_dummies(data['Company']).astype(float).values.tolist()\n",
    "data['tv'] = tr_tv.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 9235\n",
      "Test size: 486\n"
     ]
    }
   ],
   "source": [
    "# Train-dev-test split\n",
    "\n",
    "train_dataset_v = data.sample(frac=0.95, random_state=1234)\n",
    "test_dataset_v = data.drop(train_dataset_v.index)\n",
    "\n",
    "print ('Train size:', len(train_dataset_v.index))\n",
    "print ('Test size:', len(test_dataset_v.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================== Train Data: ==================================\n",
      "RMSE =  0.027232685386578026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svr_model = SVR()\n",
    "\n",
    "X = ((train_dataset_v[['r_index', 'MFI', 'ForPE', 'SIP']]\n",
    "     ).join(train_dataset_v['tv'].apply(pd.Series).add_prefix('tv_'))\n",
    "    ).join(train_dataset_v['co_dummy'].apply(pd.Series).add_prefix('co_'))\n",
    "y = train_dataset_v['r_stock']\n",
    "\n",
    "svr_model.fit(X, y)\n",
    "\n",
    "y_train = svr_model.predict(X)\n",
    "\n",
    "print(\"================================== Train Data: ==================================\")\n",
    "RMSE = compute_rmse(y_train, y)\n",
    "print(\"RMSE = \", RMSE)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================== Test Data: ==================================\n",
      "RMSE =  0.029775049558993457\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test = ((test_dataset_v[['r_index', 'MFI', 'ForPE', 'SIP']]\n",
    "     ).join(test_dataset_v['tv'].apply(pd.Series).add_prefix('tv_'))\n",
    "    ).join(test_dataset_v['co_dummy'].apply(pd.Series).add_prefix('co_'))\n",
    "y_true = test_dataset_v['r_stock']\n",
    "\n",
    "y_test = svr_model.predict(X_test)\n",
    "\n",
    "print(\"================================== Test Data: ==================================\")\n",
    "RMSE = compute_rmse(y_test, y_true)\n",
    "print(\"RMSE = \", RMSE)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
